\subsection{Threads domain}

We also encoded a simple assembly-like abstract machine with a fork/join primitive as a planning domain, which we call the ``threads'' domain.
Its predicates are used to indicate values of data and to indicate a program's instruction sequence, and
its actions represent the language's evaluation rules. For example, \texttt{value ?name ?value} indicates the value of a variable with a given name. Each instruction is encoded as both a predicate (its representation in the instruction stream) and an action (which specifies its semantics).
%
\begin{itemize}
	\item \texttt{set ?me ?next ?name ?value} - Assigns a value to a variable. \texttt{me} is the label for this instruction (all instructions share this pattern), \texttt{next} is the label of the next instruction to evaluate after this one. The increment, decrement, load, atomic-exchange, and atomic-add instructions are similar.
	\item \texttt{branch ?me ?name ?iftrue ?iffalse} - Flow control. Instead of \texttt{next}, there are two next instructions, selected between depending whether the \texttt{value name n0} fact exists (i.e., \texttt{name == 0}).
	\item \texttt{exit ?me} - Terminates execution of the current thread (or program).
	\item \texttt{fork ?me ?next ?child1 ?child2} - Runs \texttt{child1} and \texttt{child2} to completion (both must \texttt{exit}), and then advances to \texttt{next}.
\end{itemize}
%
Finally, \texttt{eval ?instruction ?out} represents the program counter; its first argument is the label associated with the next instruction to be executed, and its second argument is the ``destination'' label, a special label used to identify which threads have terminated. There are as many \texttt{eval} tokens as currently-running threads.

The first set of problems in the threads domain demonstrates a simple data race between two threads---two interleaving threads attempting to increment a shared variable can nondeterministically produce different results. The initial state of the planning problem expresses the following program:
%
	\begin{center} \small
	\begin{tabular}{ll}
	\multicolumn{2}{c}{\texttt{x = 0;~~~~~~~~~~~~~~~~~~}} \\
	\multicolumn{2}{c}{\texttt{fork(thread1, thread2);}} \\
	& \\
	\texttt{thread1() \{} & \texttt{thread2() \{} \\
	\texttt{~~~~temp0 = x;\qquad} & \texttt{~~~~temp1 = x;} \\
	\texttt{~~~~temp0++;} & \texttt{~~~~temp1++;} \\
	\texttt{~~~~x = temp0;} & \texttt{~~~~x = temp1;} \\
	\texttt{\}} & \texttt{\}} \\
	\end{tabular}
	\end{center}
%
In this example, possible values for \texttt{x} at the end are 1 and 2. The problems with filenames starting in \texttt{prob1} demonstrate this problem; those with filenames starting in \texttt{prob2} and \texttt{prob3} are elaborations on it.

\subsection{Verifying synchronisation algorithms}

Next, we used planners to verify several different algorithms for {\em synchronisation} -- the problem of protecting designated ``critical sections'' of execution from unsafe concurrent access. Mutual exclusion algorithms are characterised by three properties: {\em mutual exclusion}, {\em bounded waiting}, and {\em progress}~\cite{de0u}.

\paragraph{Mutual Exclusion}
An algorithm that provides mutual exclusion does not allow multiple threads to be executing in the critical section simultaneously. To ensure that an algorithm provides mutual exclusion, we write programs of the following form, in which both threads modify a \texttt{num\_in\_section} counter to indicate when they're in the critical section:
%
\begin{center} \small
\begin{tabular}{ll}
\multicolumn{2}{c}{\texttt{num\_in\_section = 0;~~~~~~}} \\
\multicolumn{2}{c}{\texttt{fork(thread1, thread2);}} \\
& \\
\texttt{thread1() \{} & \texttt{thread2() \{} \\
\texttt{~~~~while (true) \{} & \texttt{~~~~while (true) \{} \\
\texttt{~~~~~~~~\em lock\_sequence();} & \texttt{~~~~~~~~\em lock\_sequence();} \\
\texttt{~~~~~~~~num\_in\_section++;} & \texttt{~~~~~~~~num\_in\_section++;} \\
\texttt{~~~~~~~~num\_in\_section--;} & \texttt{~~~~~~~~num\_in\_section--;} \\
\texttt{~~~~~~~~\em unlock\_sequence();\qquad} & \texttt{~~~~~~~~\em unlock\_sequence();} \\
\texttt{~~~~\}} & \texttt{~~~~\}} \\
\texttt{\}} & \texttt{\}} \\
\end{tabular}
\end{center}
%
We then set the goal statement to be \texttt{num\_in\_section == 2}. When a complete planner cannot plan for this fact, it guarantees that no execution interleaving exists in which both threads are in the section at once -- in other words, that mutual exclusion is satisfied.

We add the infinite loop in each thread's body to test the unlock sequence as well as the lock sequence, in case a broken algorithm requires multiple iterations before failing. One particular strength of planners shines here: their ability to deal with cyclic state spaces. In the blocks world, no planner worth its salt would get stuck evaluating the infinite plan ``pick up block A, stack A on B, pick up block A, \dots'', and likewise, here the planners easily cope with the infinite loops in each thread.

\paragraph{Bounded Waiting}
An algorithm that provides bounded waiting does not allow any thread to be ``starved'' while waiting for the lock; that is, once a thread has expressed interest in acquiring the lock, there exists some finite number $N$ such that other threads can perform no more than $N$ subsequent operations on the lock before the first thread acquires it.
		To ensure bounded waiting, we write programs of the following form, in which \texttt{thread1} indicates when it has ``expressed interest in the lock'', and \texttt{thread2} counts the number of times it acquires the lock since then:
	\begin{center} \small
	\begin{tabular}{ll}
	\multicolumn{2}{c}{\texttt{thread1\_waiting = false;}} \\
	\multicolumn{2}{c}{\texttt{thread2\_iters = 0;~~~~~~~}} \\
	\multicolumn{2}{c}{\texttt{fork(thread1, thread2);~~}} \\
	& \\
	\texttt{thread1() \{} & \texttt{thread2() \{} \\
	\texttt{~~~~while (true) \{} & \texttt{~~~~while (true) \{} \\
	\texttt{~~~~~~~~\em lock\_express\_interest\_step();\qquad} & \texttt{~~~~~~~~\em lock\_sequence();} \\
	\texttt{~~~~~~~~thread1\_waiting = true;} & \texttt{~~~~~~~~thread2\_iters++;} \\
	\texttt{~~~~~~~~thread2\_iters = 0;} & \texttt{~~~~~~~~\em unlock\_sequence();} \\
     \texttt{~~~~~~~~\em lock\_acquire\_step();} & \texttt{~~~~\}} \\
	 \texttt{~~~~~~~~thread1\_waiting = false;} & \texttt{\}} \\
	\texttt{~~~~~~~~\em unlock\_sequence();} & \\
	\texttt{~~~~\}} & \\
	\texttt{\}} & \\
	\end{tabular}
	\end{center}
		We then set the goal state to \texttt{thread1\_waiting \&\& thread2\_iters == $N$} for some fixed $N$. If a complete planner cannot plan for this goal, the algorithm satisfies bounded waiting for $N$. If a planner does find a plan for arbitrarily large $N$, bounded waiting is not satisfied.

		This setup also assumes that the lock sequence can be subdivided into the two subsequences denoted above. If there exists no such split, we say that the algorithm does not provide bounded waiting.\footnote{
		This nonexistence can be verified by enumerating all possible instructions in the lock sequence before which to put the new steps. In our test suite, we are not exhaustive about this, though it is possible.}

\paragraph{Progress}
An algorithm that provides progress guarantees that for all $N$, with $N$ operations on the lock, there exists some $M$ number of instructions such that no execution trace longer than $M$ achieves fewer than $N$ operations. (In other words, all operations complete in finite time; there can be no livelocks or deadlocks on a single lock.)

		We could not devise a way to verify progress using the planning technology we had at hand. However, we imagine a hypothetical planner which could perform analysis on the state-space graph to provide this. In planning terminology, progress is stated as follows:
		\begin{itemize}
			\item For all reachable states $S$, there exists a plan from $S$ that acquires+releases the lock an additional time (no deadlock), and
			\item Given some $N$, there exists a finite $M$ such that all plans of length $M$ acquire+release the lock $N$ times (no livelock).
		\end{itemize}

\subsubsection{Results}

Using test cases following the above schemes, we verified or generated counterexamples for mutual exclusion and bounded waiting for four well-known synchronisation algorithms.\footnote{
Pseudocode sketches of each of these algorithms can be found on the wikipedia articles of the same name.}

\begin{itemize}
	\item {\bf Dekker's algorithm.} Demonstrated in the test cases that start with \texttt{prob4}. We verify mutual exclusion and refute bounded waiting.
	\item {\bf Peterson's algorithm.} Demonstrated in the test cases that start with \texttt{prob7}. We verify mutual exclusion and bounded waiting.
	\item {\bf Spinlock (atomic-exchange).} Demonstrated in the test cases that start with \texttt{prob8}. We verify mutual exclusion and refute bounded waiting.
	\item {\bf Lamport's bakery algorithm (atomic-add).} Demonstrated in the test cases that start with \texttt{prob9}. We verify mutual exclusion and bounded waiting.
\end{itemize}

We also include a broken algorithm, \texttt{prob51-broken-lock.pddl}, for which we can even refute mutual exclusion (i.e., the planner can plan for \texttt{num\_in\_section == 2}).

\subsection{Compiler}

To help test our shared memory concurrency domain and to demonstrate
its generality, we built a compiler for a very simple imperative
language with standard structured programming constructs. The compiler
is fairly simple, but makes it vastly nicer to create problems for our
imperative concurrency machine domain.
Appendix~\ref{sec:appendix} includes a motivating example for the compiler:
in Figure
\ref{fig:dekker-code}, we show part of the source program we use for
testing Dekker's algorithm, and in Figure \ref{fig:dekker-asm} we show the
corresponding output problem for our domain. Clearly, the source is
much easier to work with.

The compiler can be invoked with a description of the goal state of
the output program. The goal state can include the program having run
to completion and values for global variables.
